https://learn.microsoft.com/en-us/training/modules/generate-batch-predictions-fabric/1-introduction
https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/08d-data-science-batch.html 
===================
Organisations uses analytics platforms to build large scale data analytics solutions that generate insights and drive success. 
https://learn.microsoft.com/en-us/training/modules/examine-components-of-modern-data-warehouse/
A conventional data warehousing solution typically involves copying data from transactional data stores into a relational database with a schema that's optimized for querying and building multi-dimensional models.
Data lakehouse solutions on the other hand, are used with large volumes of data in multiple formats, which is data loaded or captured in real-time streams and stored in data-lake from which distributed processing engines like Apache Spark are used to process it.
Provision of Azure Synapse Analytics and use it to ingest, process and query data
Data Ingestion and Processing (ETL)
Analytical data store  (file-system, data lakehouses or lake databases)
Analytical data model (while data analysts and data scientisits can work with the data directly) in the analytical data store, it is common to create one or more data models that pre-aggregate the data to make it easier to produce reports, dashboards, and interactive visualisations. 
Often, these data models are described as cubes, in whcih numeric data values are aggregated across one or more dimensions (i.e. determine total sales by product and region). The model encapsulates the relationships between data values and dimensional entitites to support "drill-up" / "drill-down" analysis.
https://learn.microsoft.com/en-us/training/modules/examine-components-of-modern-data-warehouse/3-data-ingestion-pipelines
A data warehouse is a relational database in which the data is stored in a schema that is optimized for data analytics rather than transactional workloads. Commonly, the data from a transactional store is transformed into a schema in which numeric values are stored in central fact tables, which are related to one or more dimension tables that represent entities by which the data can be aggregated. For example a fact table might contain sales order data, which can be aggregated by customer, product, store, and time dimensions (enabling you, for example, to easily find monthly total sales revenue by product for each store). This kind of fact and dimension table schema is called a star schema; though it's often extended into a snowflake schema by adding additional tables related to the dimension tables to represent dimensional hierarchies (for example, product might be related to product categories). A data warehouse is a great choice when you have transactional data that can be organized into a structured schema of tables, and you want to use SQL to query them.

A data lake is a file store, usually on a distributed file system for high performance data access. Technologies like Spark or Hadoop are often used to process queries on the stored files and return data for reporting and analytics. These systems often apply a schema-on-read approach to define tabular schemas on semi-structured data files at the point where the data is read for analysis, without applying constraints when it's stored. Data lakes are great for supporting a mix of structured, semi-structured, and even unstructured data that you want to analyze without the need for schema enforcement when the data is written to the store.
You can use a hybrid approach that combines features of data lakes and data warehouses in a lake database or data lakehouse. The raw data is stored as files in a data lake, and a relational storage layer abstracts the underlying files and expose them as tables, which can be queried using SQL. SQL pools in Azure Synapse Analytics include PolyBase, which enables you to define external tables based on files in a data lake (and other sources) and query them using SQL. Synapse Analytics also supports a Lake Database approach in which you can use database templates to define the relational schema of your data warehouse, while storing the underlying data in data lake storage â€“ separating the storage and compute for your data warehousing solution. Data lakehouses are a relatively new approach in Spark-based systems, and are enabled through technologies like Delta Lake; which adds relational storage capabilities to Spark, so you can define tables that enforce schemas and transactional consistency, support batch-loaded and streaming data sources, and provide a SQL API for querying.
https://microsoftlearning.github.io/DP-900T00A-Azure-Data-Fundamentals/Instructions/Labs/dp900-04-synapse-lab.html
https://microsoftlearning.github.io/DP-900T00A-Azure-Data-Fundamentals/Instructions/Labs/dp900-04b-fabric-lake-lab.html

===============
https://learn.microsoft.com/en-us/training/modules/explore-roles-responsibilities-world-of-data/2-explore-job-roles
DE also responsible for ensuring the privacy of data is maintained within the cloud and spanning from an on-premises to the cloud data stores. 
They own the management and monitoring of data pipeline to ensure that data loads perform as expected.


