Tables in MF are based on Delta Lake storage format. 
https://learn.microsoft.com/en-gb/training/modules/work-delta-lake-tables-fabric/
Tables >> SalesOrders >> Log (JSON fomat) and PARQUET

Creating a delta table from a dataframe
#Load a file into a dataframe
df = spark.read.load('Files/', format='csv', header=True)
#Save the dataframe as a delta table
df.write.format("delta").saveAsTable("mytable")
#Saving to External Tables
df.write.format("delta").saveAsTable("myexternaltable", path="abfss://.....")
==============================================
Create a Table definition in the metastore using DataTableBuilder API
from delta.tables import *
DeltaTable.create(spark) \
	.tableName("products") \
	.addColumn("Productid", "INT") \
	.addColumn("ProductName", "STRING") \
	.addColumn("Category", "STRING") \
	.addColumn("Price", "FLOAT") \
	.execute()
https://learn.microsoft.com/en-gb/training/modules/work-delta-lake-tables-fabric/3-create-delta-tables
delta_path = "Files/mydatable"
df.write.format("delta").save(delta_path)
new_df.write.format("delta").mode("overwrite").save(delta_path)
#Append
new_rows_df.write.format("delta").mode("append").save(delta_path)
https://learn.microsoft.com/en-gb/training/modules/work-delta-lake-tables-fabric/4-work-delta-data
#######
from pyspark.sql.types import *
from pyspark.sql.functions import *

#Create a stream that reads JSON data from a folder
inputPath = 'Files/streamingdata/'
jsonSchema = StructType([
    StructField("device", StringType(), False),
    StructField("status", StringType(), False)
])
stream_df = spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger", 1).json(inputPath)

# Write the stream to a delta table
table_path = 'Files/delta/devicetable'
checkpoint_path = 'Files/delta/checkpoint'
delta_stream = stream_df.writeStream.format("delta").option("checkpointLocation", checkpoint_path).start(table_path)
	
delta_stream.stop()

https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/03-delta-lake.html

